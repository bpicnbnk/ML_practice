# 梯度下降法

## 矩阵方式描述

这一部分主要讲解梯度下降法的矩阵方式表述，相对于3.3.1的代数法，要求有一定的矩阵分析的基础知识，尤其是矩阵求导的知识。

1. 先决条件： 优化模型的假设函数和损失函数。对于线性回归，假设函数$h_\theta(x_1, x_2, ...x_n) = \theta_0 + \theta_{1}x_1 + ... + \theta_{n}x_{n}$的矩阵表达方式为：
   $$h_\mathbf{\theta}(\mathbf{X}) = \mathbf{X\theta}$$
   其中， 假设函数$h_\mathbf{\theta}(\mathbf{X})$为mx1的向量,$\mathbf{\theta}$为(n+1)x1的向量，里面有n+1个代数法的模型参数。$\mathbf{X}$为mx(n+1)维的矩阵。m代表样本的个数，n+1代表样本的特征数。

   损失函数的表达式为：$J(\mathbf\theta) = \frac{1}{2}(\mathbf{X\theta} - \mathbf{Y})^T(\mathbf{X\theta} - \mathbf{Y})$, 其中$\mathbf{Y}$是样本的输出向量，维度为mx1.

2. 算法相关参数初始化: $\theta$向量可以初始化为默认值，或者调优后的值。算法终止距离$\varepsilon$，步长$\alpha$和3.3.1比没有变化。

3. 算法过程：

   1）确定当前位置的损失函数的梯度，对于$\theta$向量,其梯度表达式如下：$\frac{\partial}{\partial\mathbf\theta}J(\mathbf\theta)$

   2）用步长乘以损失函数的梯度，得到当前位置下降的距离，即$\alpha\frac{\partial}{\partial\theta}J(\theta)$对应于前面登山例子中的某一步。

   3）确定$\mathbf\theta$向量里面的每个值,梯度下降的距离都小于$\varepsilon$，如果小于$\varepsilon$则算法终止，当前$\mathbf\theta$向量即为最终结果。否则进入步骤4.

   4）更新$\theta$向量，其更新表达式如下。更新完毕后继续转入步骤1.
   $$\mathbf\theta= \mathbf\theta - \alpha\frac{\partial}{\partial\theta}J(\mathbf\theta)$$

   　

还是用线性回归的例子来描述具体的算法过程。

损失函数对于$\theta$向量的偏导数计算如下：

   $\frac{\partial}{\partial\mathbf\theta}J(\mathbf\theta) = \mathbf{X}^T(\mathbf{X\theta} - \mathbf{Y})$

步骤4中$\theta$向量的更新表达式如下：$\mathbf\theta= \mathbf\theta - \alpha\mathbf{X}^T(\mathbf{X\theta} - \mathbf{Y})$

对于3.3.1的代数法，可以看到矩阵法要简洁很多。这里面用到了矩阵求导链式法则，和两个矩阵求导的公式。

　

这里面用到了矩阵求导链式法则，和两个个矩阵求导的公式。

   公式1：$\frac{\partial}{\partial\mathbf{x}}(\mathbf{x^Tx}) =2\mathbf{x}\;\;x为向量$

   公式2：$\nabla_Xf(AX+B) = A^T\nabla_Yf,\;\; Y=AX+B,\;\;f(Y)为标量$

如果需要熟悉矩阵求导建议参考张贤达的《矩阵分析与应用》一书。

 
## 梯度下降的算法调优

在使用梯度下降时，需要进行调优。哪些地方需要调优呢？

1. 算法的步长选择。在前面的算法描述中，我提到取步长为1，但是实际上取值取决于数据样本，可以多取一些值，从大到小，分别运行算法，看看迭代效果，如果损失函数在变小，说明取值有效，否则要增大步长。前面说了。步长太大，会导致迭代过快，甚至有可能错过最优解。步长太小，迭代速度太慢，很长时间算法都不能结束。所以算法的步长需要多次运行后才能得到一个较为优的值。

2. 算法参数的初始值选择。 初始值不同，获得的最小值也有可能不同，因此梯度下降求得的只是局部最小值；当然如果损失函数是凸函数则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。

3. 归一化。由于样本不同特征的取值范围不一样，可能导致迭代很慢，为了减少特征取值的影响，可以对特征数据归一化，也就是对于每个特征x，求出它的期望$\overline{x}$和标准差std(x)，然后转化为：

   $$\frac{x - \overline{x}}{std(x)}$$

   这样特征的新期望为0，新方差为1，迭代速度可以大大加快。
## 梯度下降法大家族（BGD，SGD，MBGD）
### 批量梯度下降法（Batch Gradient Descent）

批量梯度下降法，是梯度下降法最常用的形式，具体做法也就是在更新参数时使用所有的样本来进行更新，这个方法对应于前面3.3.1的线性回归的梯度下降算法，也就是说3.3.1的梯度下降算法就是批量梯度下降法。   

$\theta_i = \theta_i - \alpha\sum\limits_{j=1}^{m}(h_\theta(x_0^{(j)}, x_1^{(j)}, ...x_n^{(j)}) - y_j)x_i^{(j)}$

由于我们有m个样本，这里求梯度的时候就用了所有m个样本的梯度数据。
### 随机梯度下降法（Stochastic Gradient Descent）

随机梯度下降法，其实和批量梯度下降法原理类似，区别在与求梯度时没有用所有的m个样本的数据，而是仅仅选取一个样本j来求梯度。对应的更新公式是：

$\theta_i = \theta_i - \alpha (h_\theta(x_0^{(j)}, x_1^{(j)}, ...x_n^{(j)}) - y_j)x_i^{(j)}$

随机梯度下降法，和4.1的批量梯度下降法是两个极端，一个采用所有数据来梯度下降，一个用一个样本来梯度下降。自然各自的优缺点都非常突出。对于训练速度来说，随机梯度下降法由于每次仅仅采用一个样本来迭代，训练速度很快，而批量梯度下降法在样本量很大的时候，训练速度不能让人满意。对于准确度来说，随机梯度下降法用于仅仅用一个样本决定梯度方向，导致解很有可能不是最优。对于收敛速度来说，由于随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。

那么，有没有一个中庸的办法能够结合两种方法的优点呢？有！这就是4.3的小批量梯度下降法。
### 小批量梯度下降法（Mini-batch Gradient Descent）

小批量梯度下降法是批量梯度下降法和随机梯度下降法的折衷，也就是对于m个样本，我们采用x个样子来迭代，$1<x<m$。一般可以取x=10，当然根据样本的数据，可以调整这个x的值。对应的更新公式是：

$\theta_i = \theta_i - \alpha \sum\limits_{j=t}^{t+x-1}(h_\theta(x_0^{(j)}, x_1^{(j)}, ...x_n^{(j)}) - y_j)x_i^{(j)}$
## 梯度下降法和其他无约束优化算法的比较

在机器学习中的无约束优化算法，除了梯度下降以外，还有前面提到的最小二乘法，此外还有牛顿法和拟牛顿法。

梯度下降法和最小二乘法相比，梯度下降法需要选择步长，而最小二乘法不需要。梯度下降法是迭代求解，最小二乘法是计算解析解。如果样本量不算很大，且存在解析解，最小二乘法比起梯度下降法要有优势，计算速度很快。但是如果样本量很大，用最小二乘法由于需要求一个超级大的逆矩阵，这时就很难或者很慢才能求解解析解了，使用迭代的梯度下降法比较有优势。

梯度下降法和牛顿法/拟牛顿法相比，两者都是迭代求解，不过梯度下降法是梯度求解，而牛顿法/拟牛顿法是用二阶的海森矩阵的逆矩阵或伪逆矩阵求解。相对而言，使用牛顿法/拟牛顿法收敛更快。但是每次迭代的时间比梯度下降法长。

# 最小二乘法

## 多个样本特征的线性拟合。

拟合函数表示为 $h_\theta(x_1, x_2, ...x_n) = \theta_0 + \theta_{1}x_1 + ... + \theta_{n}x_{n}$, 其中$\theta_i $ (i = 0,1,2... n)为模型参数，$x_i $ (i = 0,1,2... n)为每个样本的n个特征值。这个表示可以简化，我们增加一个特征$x_0 = 1 $ ，这样拟合函数表示为：

$$h_\theta(x_0, x_1, ...x_n) = \sum\limits_{i=0}^{n}\theta_{i}x_{i}$$

损失函数表示为：

$$J(\theta_0, \theta_1..., \theta_n) = \sum\limits_{j=1}^{m}(h_\theta(x_0^{(j)}), x_1^{(j)}, ...x_n^{(j)})) - y^{(j)}))^2 = \sum\limits_{j=1}^{m}(\sum\limits_{i=0}^{n}\theta_{i}x_{i}^{(j)}- y^{(j)})^2$$

利用损失函数分别对$\theta_i(i=0,1,...n)$求导,并令导数为0可得：

$\sum\limits_{j=0}^{m}(\sum\limits_{i=0}^{n}(\theta_{i}x_{i}^{(j)} - y^{(j)})x_i^{(j)}$ = 0   (i=0,1,...n)

这样我们得到一个N+1元一次方程组，这个方程组有N+1个方程，求解这个方程，就可以得到所有的N+1个未知的$\theta$。

这个方法很容易推广到多个样本特征的非线性拟合。原理和上面的一样，都是用损失函数对各个参数求导取0，然后求解方程组得到参数值。这里就不累述了。

 
## 最小二乘法的矩阵法解法

矩阵法比代数法要简洁，且矩阵运算可以取代循环，所以现在很多书和机器学习库都是用的矩阵法来做最小二乘法。

这里用上面的多元线性回归例子来描述矩阵法解法。

假设函数$h_\theta(x_1, x_2, ...x_n) = \theta_0 + \theta_{1}x_1 + ... + \theta_{n-1}x_{n-1}$的矩阵表达方式为：

$$h_\mathbf{\theta}(\mathbf{x}) = \mathbf{X\theta}$$

其中， 假设函数$h_\mathbf{\theta}(\mathbf{X})$为mx1的向量,$\mathbf{\theta}$为nx1的向量，里面有n个代数法的模型参数。$\mathbf{X}$为mxn维的矩阵。m代表样本的个数，n代表样本的特征数。

损失函数定义为$J(\mathbf\theta) = \frac{1}{2}(\mathbf{X\theta} - \mathbf{Y})^T(\mathbf{X\theta} - \mathbf{Y})$

其中$\mathbf{Y}$是样本的输出向量，维度为mx1. $\frac{1}{2}$在这主要是为了求导后系数为1，方便计算。

根据最小二乘法的原理，我们要对这个损失函数对$\mathbf{\theta}$向量求导取0。结果如下式：

$$\frac{\partial}{\partial\mathbf\theta}J(\mathbf\theta) = \mathbf{X}^T(\mathbf{X\theta} - \mathbf{Y}) = 0$$

这里面用到了矩阵求导链式法则，和两个个矩阵求导的公式。

   公式1：$\frac{\partial}{\partial\mathbf{x}}(\mathbf{x^Tx}) =2\mathbf{x}\;\;x为向量$

   公式2：$\nabla_Xf(AX+B) = A^T\nabla_Yf,\;\; Y=AX+B,\;\;f(Y)为标量$

对上述求导等式整理后可得：

$$ \mathbf{X^{T}X\theta} = \mathbf{X^{T}Y}$$

两边同时左乘$(\mathbf{X^{T}X})^{-1}$可得：

$$\mathbf{\theta} = (\mathbf{X^{T}X})^{-1}\mathbf{X^{T}Y}$$

这样我们就一下子求出了$\theta$向量表达式的公式，免去了代数法一个个去求导的麻烦。只要给了数据,我们就可以用$\mathbf{\theta} = (\mathbf{X^{T}X})^{-1}\mathbf{X^{T}Y}$算出$\theta$。

 
## 最小二乘法的局限性和适用场景   

从上面可以看出，最小二乘法适用简洁高效，比梯度下降这样的迭代法似乎方便很多。但是这里我们就聊聊最小二乘法的局限性。

首先，最小二乘法需要计算$\mathbf{X^{T}X}$的逆矩阵，有可能它的逆矩阵不存在，这样就没有办法直接用最小二乘法了，此时梯度下降法仍然可以使用。当然，我们可以通过对样本数据进行整理，去掉冗余特征。让$\mathbf{X^{T}X}$的行列式不为0，然后继续使用最小二乘法。

第二，当样本特征n非常的大的时候，计算$\mathbf{X^{T}X}$的逆矩阵是一个非常耗时的工作（nxn的矩阵求逆），甚至不可行。此时以梯度下降为代表的迭代法仍然可以使用。那这个n到底多大就不适合最小二乘法呢？如果你没有很多的分布式大数据计算资源，建议超过10000个特征就用迭代法吧。或者通过主成分分析降低特征的维度后再用最小二乘法。

第三，如果拟合函数不是线性的，这时无法使用最小二乘法，需要通过一些技巧转化为线性才能使用，此时梯度下降仍然可以用。

第四，讲一些特殊情况。当样本量m很少，小于特征数n的时候，这时拟合方程是欠定的，常用的优化方法都无法去拟合数据。当样本量m等于特征数n的时候，用方程组求解就可以了。当m大于n时，拟合方程是超定的，也就是我们常用与最小二乘法的场景了。